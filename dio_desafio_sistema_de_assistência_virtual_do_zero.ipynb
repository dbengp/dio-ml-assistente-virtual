{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhBhECqXlUCinz52wHnk2t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbengp/dio-ml-assistente-virtual/blob/main/dio_desafio_sistema_de_assist%C3%AAncia_virtual_do_zero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desafio DIO: desenvolver um sistema de assistência virtual, utilizando PLN (Processamento de Linguagem Natural) com text to speech e speech to text."
      ],
      "metadata": {
        "id": "7C13Bwgf_o2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Configura-se o ambiente no Colab instalando as bibliotecas necessárias:"
      ],
      "metadata": {
        "id": "L3o98OTXASbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "import pyttsx3\n",
        "import webbrowser\n",
        "import wikipediaapi\n",
        "import googlemaps"
      ],
      "metadata": {
        "id": "agvYEfFPR1mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implementa-se o Módulo Text-to-Speech (TTS) para converter texto em áudio:"
      ],
      "metadata": {
        "id": "HojQ3gDgBHhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyttsx3\n",
        "\n",
        "def text_to_speech(text):\n",
        "    engine = pyttsx3.init()\n",
        "    engine.say(text)\n",
        "    engine.runAndWait()\n",
        "\n",
        "# Exemplo de uso\n",
        "text_to_speech(\"Olá, como posso ajudar você?\")"
      ],
      "metadata": {
        "id": "QvD8f6fXR7L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implementa-se o Módulo Speech-to-Text (STT) para converter fala em texto:"
      ],
      "metadata": {
        "id": "GfXBIaf2DdoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "def speech_to_text():\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        print(\"Diga algo...\")\n",
        "        audio = recognizer.listen(source)\n",
        "        try:\n",
        "            text = recognizer.recognize_google(audio, language=\"pt-BR\")\n",
        "            print(f\"Você disse: {text}\")\n",
        "            return text\n",
        "        except sr.UnknownValueError:\n",
        "            print(\"Não entendi o que você disse.\")\n",
        "            return None\n",
        "        except sr.RequestError:\n",
        "            print(\"Erro ao conectar ao serviço de reconhecimento.\")\n",
        "            return None\n",
        "\n",
        "# Exemplo de uso\n",
        "comando = speech_to_text()"
      ],
      "metadata": {
        "id": "VWTGnlJZSDEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4. Implementa-se o comandos de voz: este módulo interpreta o texto e executa ações automatizadas."
      ],
      "metadata": {
        "id": "JpOMCI_DQiuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import webbrowser\n",
        "import wikipediaapi\n",
        "import googlemaps\n",
        "\n",
        "# Configurações\n",
        "wikipedia = wikipediaapi.Wikipedia('pt')\n",
        "gmaps = googlemaps.Client(key='KEY_API_GOOGLE_MAPS')\n",
        "\n",
        "def executar_comando(comando):\n",
        "    if \"pesquisar na wikipedia\" in comando:\n",
        "        termo = comando.replace(\"pesquisar na wikipedia\", \"\").strip()\n",
        "        resultado = wikipedia.page(termo).summary\n",
        "        print(resultado)\n",
        "        text_to_speech(resultado)\n",
        "    elif \"abrir youtube\" in comando:\n",
        "        webbrowser.open(\"https://www.youtube.com\")\n",
        "    elif \"localizar farmácia\" in comando:\n",
        "        localizacao = gmaps.places(\"farmácia\", location=(SUA_LATITUDE, SUA_LONGITUDE), radius=5000)\n",
        "        if localizacao['results']:\n",
        "            endereco = localizacao['results'][0]['formatted_address']\n",
        "            print(f\"A farmácia mais próxima está em: {endereco}\")\n",
        "            text_to_speech(f\"A farmácia mais próxima está em: {endereco}\")\n",
        "        else:\n",
        "            print(\"Nenhuma farmácia encontrada.\")\n",
        "            text_to_speech(\"Nenhuma farmácia encontrada.\")\n",
        "    else:\n",
        "        print(\"Comando não reconhecido.\")\n",
        "        text_to_speech(\"Desculpe, não entendi o comando.\")\n",
        "\n",
        "# Exemplo de uso\n",
        "comando = speech_to_text()\n",
        "if comando:\n",
        "    executar_comando(comando)"
      ],
      "metadata": {
        "id": "8VwpX4maQymE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Implementa-se a integração com o Chrome para reproduzir áudio diretamente nesse navegador."
      ],
      "metadata": {
        "id": "klD2KDo-Q7yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "\n",
        "def reproduzir_audio_no_chrome(texto):\n",
        "    driver = webdriver.Chrome()\n",
        "    driver.get(f\"data:text/html,<script>var utterance = new SpeechSynthesisUtterance('{texto}'); speechSynthesis.speak(utterance);</script>\")"
      ],
      "metadata": {
        "id": "t3PFDbpdROpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ab8sfcQrRTMr"
      }
    }
  ]
}